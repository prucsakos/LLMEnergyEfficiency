{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb05ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc16c982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3e498a869040b28e3f47d160094155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c84095fddf4310ac5b8e25d68a8d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a047c585db4c9f9b6cf4dd3c61d85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3765561a86c46808cf3a199491f58f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822aaacc217410f875c45bbce92b718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155e0097c99b49f2b386f15cdfff2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6c29b9286a415384ff3996f518472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Important note on benchmarks: Each dataset have its unique properties, evaluation methods\n",
    "and structure. Thats why each dataset used in the project shall have their unique adapters.\n",
    "\"\"\"\n",
    "\n",
    "# Data Load\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"hotpotqa/hotpot_qa\", \"fullwiki\")\n",
    "#dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "#dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b484b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "F: id | 90b30172e645ff91f7171a048582eb8b\n",
      "F: question | The townhouse was a hard sell for the realtor, it was right next to a high rise what?\n",
      "F: question_concept | townhouse\n",
      "F: choices | {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['suburban development', 'apartment building', 'bus stop', 'michigan', 'suburbs']}\n",
      "F: answerKey | \n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "for f in dataset['test'].features.keys():\n",
    "    print(f\"F: {f} | {dataset['test'][f][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7dba1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 12:18:59 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'nvidia/NVIDIA-Nemotron-Nano-9B-v2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 12:19:00 [__init__.py:742] Resolved architecture: NemotronHForCausalLM\n",
      "INFO 09-12 12:19:00 [__init__.py:1820] Using max model len 131072\n",
      "INFO 09-12 12:19:00 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-12 12:19:00 [config.py:310] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
      "INFO 09-12 12:19:00 [config.py:321] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n",
      "INFO 09-12 12:19:00 [config.py:390] Setting attention block size to 672 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 09-12 12:19:00 [config.py:411] Padding mamba page size by 2.13% to ensure that mamba page size and attention page size are exactly equal.\n",
      "INFO 09-12 12:19:00 [__init__.py:3405] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:02 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:02 [core.py:76] Initializing a V1 LLM engine (v0.10.2rc3.dev13+gfdb09c77d) with config: model='nvidia/NVIDIA-Nemotron-Nano-9B-v2', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-Nano-9B-v2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-Nano-9B-v2, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:03 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m WARNING 09-12 12:19:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W912 12:19:03.752276596 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:03 [gpu_model_runner.py:2338] Starting to load model nvidia/NVIDIA-Nemotron-Nano-9B-v2...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:04 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.82it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:07 [default_loader.py:268] Loading weights took 2.92 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m INFO 09-12 12:19:08 [gpu_model_runner.py:2392] Model loading took 16.5842 GiB and 3.574749 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 91, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 183, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 84, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 263, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3031, in profile_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     = self._dummy_run(self.max_num_tokens, is_profile=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2809, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]               ^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 609, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     hidden_states = self.model(input_ids, positions, mamba_cache_params,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 223, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 412, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     hidden_states, residual = layer(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]                               ^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 195, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]     output = torch.empty_like(hidden_states)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] torch.AcceleratorError: CUDA error: no kernel image is available for execution on the device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m ERROR 09-12 12:19:08 [core.py:718] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 91, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 183, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 84, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 263, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3031, in profile_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     = self._dummy_run(self.max_num_tokens, is_profile=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2809, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m               ^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 609, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     hidden_states = self.model(input_ids, positions, mamba_cache_params,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 223, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 412, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     hidden_states, residual = layer(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m                               ^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m   File \"/home/akos.prucs@egroup.hu/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/model_executor/models/nemotron_h.py\", line 195, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m     output = torch.empty_like(hidden_states)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m torch.AcceleratorError: CUDA error: no kernel image is available for execution on the device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=408057)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m prompts = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHello, my name is\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe president of the United States is\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe capital of France is\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe future of AI is\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m ]\n\u001b[32m     12\u001b[39m sampling_params = SamplingParams(temperature=\u001b[32m0.8\u001b[39m, top_p=\u001b[32m0.95\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:282\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    279\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    286\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:493\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    491\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:134\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    133\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:111\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:602\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    601\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    610\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:448\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    445\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:729\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMEnergyEfficiency/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:782\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    781\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    783\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    784\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    787\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "# VLLM\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "model = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(\n",
    "    model=model,\n",
    "    enforce_eager=True,\n",
    "    dtype=\"auto\",\n",
    "    gpu_memory_utilization=0.3,\n",
    "    trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddafafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 12:25:17 [__init__.py:1680] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7409a32c1464f16a2c24c3d9bce6c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f067021939f40109ac8acbf0068bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[1, 15043, 29892, 590, 1024, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' asbaggerun away at yourad399999a,', token_ids=[408, 29890, 9921, 348, 3448, 472, 596, 328, 29941, 29929, 29929, 29929, 29929, 29929, 29874, 29892], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[1, 450, 6673, 310, 278, 3303, 3900, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=', and 1, g., To, 1, g., wS', token_ids=[29892, 322, 29871, 29896, 29892, 330, 1696, 1763, 29892, 29871, 29896, 29892, 330, 1696, 281, 29903], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=2, prompt='The capital of France is', prompt_token_ids=[1, 450, 7483, 310, 3444, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=', and 7.bagger\\'shave\", g, and', token_ids=[29892, 322, 29871, 29955, 29889, 29890, 9921, 29915, 845, 1351, 29908, 30010, 29892, 330, 29892, 322], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=3, prompt='The future of AI is', prompt_token_ids=[1, 450, 5434, 310, 319, 29902, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' to 78d (D1 (As and take ons2', token_ids=[304, 29871, 29955, 29947, 29881, 313, 29928, 29896, 313, 2887, 322, 2125, 373, 30010, 29879, 29906], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(prompts)\n",
    "for o in outputs:\n",
    "    print(o.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aabe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mlflow logging / experiment tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMEnergyEfficiency (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
