{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb05ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc16c982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3e498a869040b28e3f47d160094155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c84095fddf4310ac5b8e25d68a8d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a047c585db4c9f9b6cf4dd3c61d85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3765561a86c46808cf3a199491f58f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822aaacc217410f875c45bbce92b718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155e0097c99b49f2b386f15cdfff2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6c29b9286a415384ff3996f518472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Important note on benchmarks: Each dataset have its unique properties, evaluation methods\n",
    "and structure. Thats why each dataset used in the project shall have their unique adapters.\n",
    "\"\"\"\n",
    "\n",
    "# Data Load\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"hotpotqa/hotpot_qa\", \"fullwiki\")\n",
    "#dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "#dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b484b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "F: id | 90b30172e645ff91f7171a048582eb8b\n",
      "F: question | The townhouse was a hard sell for the realtor, it was right next to a high rise what?\n",
      "F: question_concept | townhouse\n",
      "F: choices | {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['suburban development', 'apartment building', 'bus stop', 'michigan', 'suburbs']}\n",
      "F: answerKey | \n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "for f in dataset['test'].features.keys():\n",
    "    print(f\"F: {f} | {dataset['test'][f][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7dba1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 12:44:50 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-10 12:44:51 [utils.py:328] non-default args: {'disable_log_stats': True, 'model': 'meta-llama/Llama-2-7b-chat-hf'}\n",
      "INFO 09-10 12:44:57 [__init__.py:745] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 12:44:57 [__init__.py:1800] Using max model len 4096\n",
      "INFO 09-10 12:44:57 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:44:58 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:44:58 [core.py:76] Initializing a V1 LLM engine (v0.10.2rc2.dev25+g0ae43dbf8) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:44:59 [parallel_state.py:1164] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m WARNING 09-10 12:44:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:00 [gpu_model_runner.py:2181] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W910 12:44:59.085141535 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:00 [gpu_model_runner.py:2213] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:00 [cuda.py:338] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:00 [weight_utils.py:347] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e6d57ac88443fda758f5b6e8f60d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:02 [default_loader.py:266] Loading weights took 1.97 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:03 [gpu_model_runner.py:2235] Model loading took 12.5524 GiB and 2.707273 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:06 [backends.py:538] Using cache directory: /home/akos.prucs@egroup.hu/.cache/vllm/torch_compile_cache/bdcedfa8b6/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:06 [backends.py:549] Dynamo bytecode transform time: 2.58 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.909 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:08 [monitor.py:34] torch.compile takes 2.58 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:09 [gpu_worker.py:276] Available KV cache memory: 71.39 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:09 [kv_cache_utils.py:864] GPU KV cache size: 146,208 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:09 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 35.70x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:12 [gpu_model_runner.py:2937] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=230388)\u001b[0;0m INFO 09-10 12:45:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 8.80 seconds\n",
      "INFO 09-10 12:45:12 [llm.py:285] Supported_tasks: ['generate']\n",
      "INFO 09-10 12:45:12 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "# VLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(\n",
    "    model=model,\n",
    "    enforce_eager=False,\n",
    "    dtype=\"auto\",\n",
    "    gpu_memory_utilization=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddafafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 12:25:17 [__init__.py:1680] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7409a32c1464f16a2c24c3d9bce6c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f067021939f40109ac8acbf0068bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[1, 15043, 29892, 590, 1024, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' asbaggerun away at yourad399999a,', token_ids=[408, 29890, 9921, 348, 3448, 472, 596, 328, 29941, 29929, 29929, 29929, 29929, 29929, 29874, 29892], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[1, 450, 6673, 310, 278, 3303, 3900, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=', and 1, g., To, 1, g., wS', token_ids=[29892, 322, 29871, 29896, 29892, 330, 1696, 1763, 29892, 29871, 29896, 29892, 330, 1696, 281, 29903], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=2, prompt='The capital of France is', prompt_token_ids=[1, 450, 7483, 310, 3444, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=', and 7.bagger\\'shave\"’, g, and', token_ids=[29892, 322, 29871, 29955, 29889, 29890, 9921, 29915, 845, 1351, 29908, 30010, 29892, 330, 29892, 322], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=3, prompt='The future of AI is', prompt_token_ids=[1, 450, 5434, 310, 319, 29902, 338], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' to 78d (D1 (As and take on’s2', token_ids=[304, 29871, 29955, 29947, 29881, 313, 29928, 29896, 313, 2887, 322, 2125, 373, 30010, 29879, 29906], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(prompts)\n",
    "for o in outputs:\n",
    "    print(o.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aabe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mlflow logging / experiment tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMEnergyEfficiency (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
