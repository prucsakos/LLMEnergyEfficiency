{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ebcc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Device count: 1\n",
      "Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition\n",
      "  Capability: (12, 0)\n",
      "  Memory (GB): 94.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akos.prucs@egroup.hu/workspace/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:287: UserWarning: \n",
      "NVIDIA RTX PRO 6000 Blackwell Workstation Edition with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA RTX PRO 6000 Blackwell Workstation Edition GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"  Capability:\", torch.cuda.get_device_capability(i))\n",
    "        print(\"  Memory (GB):\", round(torch.cuda.get_device_properties(i).total_memory / 1024**3, 2))\n",
    "else:\n",
    "    print(\"No NVIDIA GPU detected by PyTorch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89aa4d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/akos.prucs@egroup.hu/workspace/.venv/lib/python3.13/site-packages/selective_scan_cuda.cpython-313-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEab",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m      5\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:586\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:581\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    569\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    570\u001b[39m     repo_id,\n\u001b[32m    571\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m     repo_type=repo_type,\n\u001b[32m    580\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:276\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/nvidia/NVIDIA-Nemotron-Nano-9B-v2/dc376c20a64208fc2cb4667e00af485eeced8ae4/modeling_nemotron_h.py:55\u001b[39m\n\u001b[32m     50\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Copied from transformers.models.mamba.modeling_mamba2.modeling_mamba2.py with MAMBA2->NEMOTRONH,Mamba2->NemotronH\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# For Mamba2 components Mamba2->NemotronHMamba2\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_mamba_2_ssm_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mselective_state_update\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m selective_state_update\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mssd_combined\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:583\u001b[39m, in \u001b[36mis_mamba_2_ssm_available\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_package_available(\u001b[33m\"\u001b[39m\u001b[33mmamba_ssm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\n\u001b[32m    585\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m version.parse(mamba_ssm.__version__) >= version.parse(\u001b[33m\"\u001b[39m\u001b[33m2.0.4\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    586\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/mamba_ssm/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m2.2.5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mselective_scan_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m selective_scan_fn, mamba_inner_fn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmamba_simple\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmamba2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mamba2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/.venv/lib/python3.13/site-packages/mamba_ssm/ops/selective_scan_interface.py:20\u001b[39m\n\u001b[32m     16\u001b[39m     causal_conv1d_update_function = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayer_norm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _layer_norm_fwd\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselective_scan_cuda\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSelectiveScanFn\u001b[39;00m(torch.autograd.Function):\n\u001b[32m     25\u001b[39m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(ctx, u, delta, A, B, C, D=\u001b[38;5;28;01mNone\u001b[39;00m, z=\u001b[38;5;28;01mNone\u001b[39;00m, delta_bias=\u001b[38;5;28;01mNone\u001b[39;00m, delta_softplus=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     27\u001b[39m                 return_last_state=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mImportError\u001b[39m: /home/akos.prucs@egroup.hu/workspace/.venv/lib/python3.13/site-packages/selective_scan_cuda.cpython-313-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEab"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111d34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"/think\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a letter to my boss.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d014dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SPECIAL_10>System\n",
      "\n",
      "<SPECIAL_11>User\n",
      "Write a letter to my boss.\n",
      "<SPECIAL_11>Assistant\n",
      "<think>\n",
      "<SPECIAL_12>\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    temperature=0.6,\n",
    "    top_p=0.90\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    tokenized_chat,\n",
    "    max_new_tokens=1000,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "947c63e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecfa8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'workspace (3.12.11) (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/akos.prucs@egroup.hu/workspace/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import openai\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ThinkingBudgetClient:\n",
    "   def __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\n",
    "       self.base_url = base_url\n",
    "       self.api_key = api_key\n",
    "       self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "       self.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "\n",
    "\n",
    "   def chat_completion(\n",
    "       self,\n",
    "       model: str,\n",
    "       messages: List[Dict[str, Any]],\n",
    "       max_thinking_budget: int = 512,\n",
    "       max_tokens: int = 1024,\n",
    "       **kwargs,\n",
    "   ) -> Dict[str, Any]:\n",
    "       assert (\n",
    "           max_tokens > max_thinking_budget\n",
    "       ), f\"thinking budget must be smaller than maximum new tokens. Given {max_tokens=} and {max_thinking_budget=}\"\n",
    "\n",
    "\n",
    "       # 1. first call chat completion to get reasoning content\n",
    "       response = self.client.chat.completions.create(\n",
    "           model=model, messages=messages, max_tokens=max_thinking_budget, **kwargs\n",
    "       )\n",
    "       content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "       reasoning_content = content\n",
    "       if not \"</think>\" in reasoning_content:\n",
    "           # reasoning content is too long, closed with a period (.)\n",
    "           reasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\n",
    "       reasoning_tokens_len = len(\n",
    "           self.tokenizer.encode(reasoning_content, add_special_tokens=False)\n",
    "       )\n",
    "       remaining_tokens = max_tokens - reasoning_tokens_len\n",
    "       assert (\n",
    "           remaining_tokens > 0\n",
    "       ), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase the max_tokens or lower the max_thinking_budget.\"\n",
    "\n",
    "\n",
    "       # 2. append reasoning content to messages and call completion\n",
    "       messages.append({\"role\": \"assistant\", \"content\": reasoning_content})\n",
    "       prompt = self.tokenizer.apply_chat_template(\n",
    "           messages,\n",
    "           tokenize=False,\n",
    "           continue_final_message=True,\n",
    "       )\n",
    "       response = self.client.completions.create(\n",
    "           model=model, prompt=prompt, max_tokens=remaining_tokens, **kwargs\n",
    "       )\n",
    "\n",
    "\n",
    "       response_data = {\n",
    "           \"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n",
    "           \"content\": response.choices[0].text,\n",
    "           \"finish_reason\": response.choices[0].finish_reason,\n",
    "       }\n",
    "       return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5323f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name_or_path = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "client = ThinkingBudgetClient(\n",
    "   base_url=\"http://localhost:8000/v1\",  # Nano 9B v2 deployed in thinking mode\n",
    "   api_key=\"EMPTY\",\n",
    "   tokenizer_name_or_path=tokenizer_name_or_path,\n",
    ")\n",
    "\n",
    "result = client.chat_completion(\n",
    "   model=\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
    "   messages=[\n",
    "       {\"role\": \"system\", \"content\": \"You are a helpful assistant. /think\"},\n",
    "       {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "   ],\n",
    "   max_thinking_budget=32,\n",
    "   max_tokens=512,\n",
    "   temperature=0.6,\n",
    "   top_p=0.95,\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMEnergyEfficiency (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
