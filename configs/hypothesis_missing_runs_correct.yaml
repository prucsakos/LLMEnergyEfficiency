prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing (not even whitespace) may appear before <think> or after </think>.
    - The block MUST be non-empty, closed properly, and contain only reasoning steps.
    - Do NOT restate the question or include the final answer here.
    - Keep the reasoning concise (≤150 tokens), no filler, no meta-comments.

    <question>
    {question}
    </question>
    <think>
  plan_think: |
    Devise a brief high-level plan ONLY inside <think>...</think>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing may appear before <think> or after </think>.
    - The block MUST be non-empty and contain 1–5 short bullet points.
    - Do NOT include calculations, reasoning chains, or the final answer here.

    <question>
    {question}
    </question>
    <think>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Allowed text: <final>, </final>, Short answer string.
    - Nothing may appear before <final> or after </final>.
    - After <final>, write down the exact short answer and the closing </final> tag.

    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |-
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.

    RULES:
    - Compare trimmed strings exactly, case-sensitive.
    - If either side is empty, ambiguous, or contains multiple answers, reply NO.
    - Output MUST be non-empty: write YES or NO after "Judgement: ".

    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement: 
  direct: |
    Answer the question. Output ONLY inside <final>...</final>. Do not include any reasoning or explanation anywhere.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Nothing may appear before <final> or after </final>.
    - The block MUST be non-empty and contain only the minimal answer string.

    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

    COUNTING RULES:
    - Count exact-string matches after trimming whitespace.
    - If there is a tie, pick the earliest occurring answer in the provided list.
    - Your ENTIRE output must be EXACTLY one well-formed <chosen>...</chosen> block, containing the chosen answer copied verbatim.
    - Nothing may appear before <chosen> or after </chosen>.

    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>

    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

config_name: "missing_runs_correct"
datasets_: [aime_2025, hmmt_feb_2025, gpqa]
datasets: [aime_2025, hmmt_feb_2025, gpqa, prontoqa, proofwriter]

models:

  # --- DeepSeek-R1-0528-Qwen3-8B for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: DeepSeek-R1-0528-Qwen3-8B_aime_2025
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}
    datasets: [aime_2025]

  # --- DeepSeek-R1-0528-Qwen3-8B for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: DeepSeek-R1-0528-Qwen3-8B_gpqa
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}
    datasets: [gpqa]

  # --- DeepSeek-R1-0528-Qwen3-8B for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: DeepSeek-R1-0528-Qwen3-8B_hmmt_feb_2025
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}
    datasets: [hmmt_feb_2025]

  # --- DeepSeek-R1-0528-Qwen3-8B for prontoqa (missing: [0, 2048, 12000, 16192]) ---
  - name: DeepSeek-R1-0528-Qwen3-8B_prontoqa
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}
    datasets: [prontoqa]

  # --- DeepSeek-R1-0528-Qwen3-8B for proofwriter (missing: [512, 8192, 16192]) ---
  - name: DeepSeek-R1-0528-Qwen3-8B_proofwriter
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}
    datasets: [proofwriter]

  # --- GLM-4.5-Air-AWQ-4bit for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: GLM-4.5-Air-AWQ-4bit_aime_2025
    hf_repo: cpatonn/GLM-4.5-Air-AWQ-4bit
    engine: vllm
    model_family: glm
    batch_size: 2
    card: { params_B: 106, layers: 46, hidden_dim: 4096, heads: 96, arch: decoder-only MoE (n_routed_experts=128, 8 active) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Hybrid reasoning modes; 131k context per config." # :contentReference[oaicite:9]{index=9}
    datasets: [aime_2025]

  # --- Llama-3.1-8B-Instruct for gpqa (missing: [0, 2048, 8192, 12000, 16192]) ---
  - name: Llama-3.1-8B-Instruct_gpqa
    hf_repo: meta-llama/Llama-3.1-8B-Instruct
    engine: vllm
    model_family: llama
    batch_size: 128
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Fields taken from Meta-Llama-3.1-8B mirrors (identical arch dims)." # :contentReference[oaicite:10]{index=10}
    datasets: [gpqa]

  # --- Llama-3.1-8B-Instruct for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: Llama-3.1-8B-Instruct_prontoqa
    hf_repo: meta-llama/Llama-3.1-8B-Instruct
    engine: vllm
    model_family: llama
    batch_size: 128
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Fields taken from Meta-Llama-3.1-8B mirrors (identical arch dims)." # :contentReference[oaicite:10]{index=10}
    datasets: [prontoqa]

  # --- Llama-3.1-8B-Instruct for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: Llama-3.1-8B-Instruct_proofwriter
    hf_repo: meta-llama/Llama-3.1-8B-Instruct
    engine: vllm
    model_family: llama
    batch_size: 128
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Fields taken from Meta-Llama-3.1-8B mirrors (identical arch dims)." # :contentReference[oaicite:10]{index=10}
    datasets: [proofwriter]

  # --- Phi-4-mini-reasoning for aime_2025 (missing: [8192, 12000, 16192]) ---
  - name: Phi-4-mini-reasoning_aime_2025
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}
    datasets: [aime_2025]

  # --- Phi-4-mini-reasoning for gpqa (missing: [512, 8192, 16192]) ---
  - name: Phi-4-mini-reasoning_gpqa
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}
    datasets: [gpqa]

  # --- Phi-4-mini-reasoning for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000]) ---
  - name: Phi-4-mini-reasoning_hmmt_feb_2025
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}
    datasets: [hmmt_feb_2025]

  # --- Phi-4-mini-reasoning for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: Phi-4-mini-reasoning_prontoqa
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}
    datasets: [prontoqa]

  # --- Phi-4-mini-reasoning for proofwriter (missing: [0, 512, 2048, 8192, 16192]) ---
  - name: Phi-4-mini-reasoning_proofwriter
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}
    datasets: [proofwriter]

  # --- Phi-4-reasoning-plus for aime_2025 (missing: [16192]) ---
  - name: Phi-4-reasoning-plus_aime_2025
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
    datasets: [aime_2025]

  # --- Phi-4-reasoning-plus for gpqa (missing: [0, 2048, 12000, 16192]) ---
  - name: Phi-4-reasoning-plus_gpqa
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
    datasets: [gpqa]

  # --- Phi-4-reasoning-plus for hmmt_feb_2025 (missing: [512, 2048, 8192, 12000, 16192]) ---
  - name: Phi-4-reasoning-plus_hmmt_feb_2025
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
    datasets: [hmmt_feb_2025]

  # --- Phi-4-reasoning-plus for prontoqa (missing: [0, 512, 2048, 8192, 16192]) ---
  - name: Phi-4-reasoning-plus_prontoqa
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
    datasets: [prontoqa]

  # --- Phi-4-reasoning-plus for proofwriter (missing: [512, 8192, 12000, 16192]) ---
  - name: Phi-4-reasoning-plus_proofwriter
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
    datasets: [proofwriter]

  # --- Qwen3-4B-Instruct-2507 for prontoqa (missing: [512, 1024, 2048, 8192, 12000, 16192]) ---
  - name: Qwen3-4B-Instruct-2507_prontoqa
    hf_repo: Qwen/Qwen3-4B-Instruct-2507
    engine: vllm
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [0, 256, 512, 1024, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "" # :contentReference[oaicite:5]{index=5}
    datasets: [prontoqa]

  # --- Qwen3-4B-Instruct-2507 for proofwriter (missing: [0, 256, 512, 8192, 12000, 16192]) ---
  - name: Qwen3-4B-Instruct-2507_proofwriter
    hf_repo: Qwen/Qwen3-4B-Instruct-2507
    engine: vllm
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [0, 256, 512, 1024, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "" # :contentReference[oaicite:5]{index=5}
    datasets: [proofwriter]

  # --- gemma-3-12b-it for gpqa (missing: [0, 512, 8192]) ---
  - name: gemma-3-12b-it_gpqa
    hf_repo: google/gemma-3-12b-it
    engine: vllm
    model_family: gemma
    batch_size: 32
    card: { params_B: 12, layers: 40, hidden_dim: 3072, heads: 24, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:16]{index=16}
    datasets: [gpqa]

  # --- gemma-3-12b-it for prontoqa (missing: [512, 2048]) ---
  - name: gemma-3-12b-it_prontoqa
    hf_repo: google/gemma-3-12b-it
    engine: vllm
    model_family: gemma
    batch_size: 32
    card: { params_B: 12, layers: 40, hidden_dim: 3072, heads: 24, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:16]{index=16}
    datasets: [prontoqa]

  # --- gemma-3-12b-it for proofwriter (missing: [0, 8192]) ---
  - name: gemma-3-12b-it_proofwriter
    hf_repo: google/gemma-3-12b-it
    engine: vllm
    model_family: gemma
    batch_size: 32
    card: { params_B: 12, layers: 40, hidden_dim: 3072, heads: 24, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:16]{index=16}
    datasets: [proofwriter]

  # --- gemma-3-1b-it for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-1b-it_aime_2025
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}
    datasets: [aime_2025]

  # --- gemma-3-1b-it for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-1b-it_gpqa
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}
    datasets: [gpqa]

  # --- gemma-3-1b-it for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-1b-it_hmmt_feb_2025
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}
    datasets: [hmmt_feb_2025]

  # --- gemma-3-1b-it for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-1b-it_prontoqa
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}
    datasets: [prontoqa]

  # --- gemma-3-1b-it for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-1b-it_proofwriter
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}
    datasets: [proofwriter]

  # --- gemma-3-27b-it for gpqa (missing: [512, 8192, 16192]) ---
  - name: gemma-3-27b-it_gpqa
    hf_repo: google/gemma-3-27b-it
    engine: vllm
    model_family: gemma
    batch_size: 8
    card: { params_B: 27, layers: 62, hidden_dim: 5376, heads: 32, arch: gemma3_text (decoder-only; kv_heads=16) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via MLX community mirror; official repo gated." # :contentReference[oaicite:17]{index=17}
    datasets: [gpqa]

  # --- gemma-3-27b-it for prontoqa (missing: [12000, 16192]) ---
  - name: gemma-3-27b-it_prontoqa
    hf_repo: google/gemma-3-27b-it
    engine: vllm
    model_family: gemma
    batch_size: 8
    card: { params_B: 27, layers: 62, hidden_dim: 5376, heads: 32, arch: gemma3_text (decoder-only; kv_heads=16) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via MLX community mirror; official repo gated." # :contentReference[oaicite:17]{index=17}
    datasets: [prontoqa]

  # --- gemma-3-27b-it for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-27b-it_proofwriter
    hf_repo: google/gemma-3-27b-it
    engine: vllm
    model_family: gemma
    batch_size: 8
    card: { params_B: 27, layers: 62, hidden_dim: 5376, heads: 32, arch: gemma3_text (decoder-only; kv_heads=16) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via MLX community mirror; official repo gated." # :contentReference[oaicite:17]{index=17}
    datasets: [proofwriter]

  # --- gemma-3-4b-it for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-4b-it_aime_2025
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}
    datasets: [aime_2025]

  # --- gemma-3-4b-it for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3-4b-it_hmmt_feb_2025
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}
    datasets: [hmmt_feb_2025]

  # --- gemma-3-4b-it for prontoqa (missing: [8192]) ---
  - name: gemma-3-4b-it_prontoqa
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}
    datasets: [prontoqa]

  # --- gemma-3-4b-it for proofwriter (missing: [0]) ---
  - name: gemma-3-4b-it_proofwriter
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}
    datasets: [proofwriter]

  # --- gemma-3n-E2B-it for aime_2025 (missing: [0, 512, 8192, 12000, 16192]) ---
  - name: gemma-3n-E2B-it_aime_2025
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}
    datasets: [aime_2025]

  # --- gemma-3n-E2B-it for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E2B-it_gpqa
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}
    datasets: [gpqa]

  # --- gemma-3n-E2B-it for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E2B-it_hmmt_feb_2025
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}
    datasets: [hmmt_feb_2025]

  # --- gemma-3n-E2B-it for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E2B-it_prontoqa
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}
    datasets: [prontoqa]

  # --- gemma-3n-E2B-it for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E2B-it_proofwriter
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}
    datasets: [proofwriter]

  # --- gemma-3n-E4B-it for aime_2025 (missing: [0, 512, 2048, 12000, 16192]) ---
  - name: gemma-3n-E4B-it_aime_2025
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 64, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}
    datasets: [aime_2025]

  # --- gemma-3n-E4B-it for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E4B-it_gpqa
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 64, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}
    datasets: [gpqa]

  # --- gemma-3n-E4B-it for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E4B-it_hmmt_feb_2025
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 64, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}
    datasets: [hmmt_feb_2025]

  # --- gemma-3n-E4B-it for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E4B-it_prontoqa
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 64, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}
    datasets: [prontoqa]

  # --- gemma-3n-E4B-it for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gemma-3n-E4B-it_proofwriter
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 64, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}
    datasets: [proofwriter]

  # --- gpt-oss-120b-BitsAndBytes-4bit for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit_aime_2025
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}
    datasets: [aime_2025]

  # --- gpt-oss-120b-BitsAndBytes-4bit for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit_gpqa
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}
    datasets: [gpqa]

  # --- gpt-oss-120b-BitsAndBytes-4bit for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit_hmmt_feb_2025
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}
    datasets: [hmmt_feb_2025]

  # --- gpt-oss-120b-BitsAndBytes-4bit for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit_prontoqa
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}
    datasets: [prontoqa]

  # --- gpt-oss-120b-BitsAndBytes-4bit for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit_proofwriter
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}
    datasets: [proofwriter]

  # --- gpt-oss-20b for gpqa (missing: [8192, 12000, 16192]) ---
  - name: gpt-oss-20b_gpqa
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}
    datasets: [gpqa]

  # --- gpt-oss-20b for prontoqa (missing: [2048, 8192, 16192]) ---
  - name: gpt-oss-20b_prontoqa
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}
    datasets: [prontoqa]

  # --- gpt-oss-20b for proofwriter (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: gpt-oss-20b_proofwriter
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}
    datasets: [proofwriter]

  # --- phi-4 for aime_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: phi-4_aime_2025
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}
    datasets: [aime_2025]

  # --- phi-4 for gpqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: phi-4_gpqa
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}
    datasets: [gpqa]

  # --- phi-4 for hmmt_feb_2025 (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: phi-4_hmmt_feb_2025
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}
    datasets: [hmmt_feb_2025]

  # --- phi-4 for prontoqa (missing: [0, 512, 2048, 8192, 12000, 16192]) ---
  - name: phi-4_prontoqa
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}
    datasets: [prontoqa]

  # --- phi-4 for proofwriter (missing: [0, 512, 2048, 8192, 16192]) ---
  - name: phi-4_proofwriter
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 50, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}
    datasets: [proofwriter]
