prompts:
  cot_think: |
    Think through this problem step by step. Show all your reasoning, calculations, and analysis.
    Do not provide the final answer - focus only on the thinking process.

    {question}
  answer: |
    Based on your reasoning, what is the final answer? Provide only the numerical result.

    {question}

    {deliberate}
  self_eval: |-
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.

    RULES:
    - Compare trimmed strings exactly, case-sensitive.
    - If either side is empty, ambiguous, or contains multiple answers, reply NO.
    - Output MUST be non-empty: write YES or NO after "Judgement: ".

    Question: {question}
    Gold Answer: {gold}
    Candidate Answer: {candidate}
    Judgement: 
  llm_judge: |
    You are an expert mathematics evaluator. Compare these two answers:
    
    Gold standard: {gold}
    Student answer: {candidate}
    
    Question context: {question}
    
    Are these answers mathematically equivalent? Consider:
    - Same final answer regardless of format
    - Multiple choice: same letter selection
    - Numerical: same value (allow minor rounding)
    - Allow different presentation of same solution
    
    Respond: CORRECT or INCORRECT with brief reasoning.
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

    COUNTING RULES:
    - Count exact-string matches after trimming whitespace.
    - If there is a tie, pick the earliest occurring answer in the provided list.
    - Output the chosen answer directly without any additional formatting.

    Question:
    {question}
    
    Candidate Answers:
    {candidate_answers}

    Count the frequency of each answer and select the most popular one. Output only the chosen answer:

config_name: "benchmark_validation"
datasets: [aime_2025, hmmt_feb_2025]

models:

  # --- Qwen3 4B ---
  - name: Qwen3-4B-Thinking-2507
    hf_repo: Qwen/Qwen3-4B-Thinking-2507
    engine: vllm
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32QÃ—8KV) }
    think_budgets: [8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "262k context; thinking mode." # :contentReference[oaicite:4]{index=4}

  - name: gpt-oss-20b
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}

  - name: gemma-3-4b-it
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, do_sample: true, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}
