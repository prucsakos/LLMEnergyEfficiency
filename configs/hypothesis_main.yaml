# Hypothesis 1: Compute Allocation
# For fixed FLOPs, small+more reasoning can rival big+less reasoning, with different latency/memory trade-offs
# This config tests the trade-off between model size and reasoning depth

config_name: "main_>40B_models_small_benchmarks"

prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.
    Document your analytical process ONLY inside <scratchpad>...</scratchpad>.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <scratchpad>
  plan_think: |
    Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <plan>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.
    Be precise and compact. Do not include any explanation or reasoning.
    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement:
  direct: |
    Answer the question. Output ONLY inside <final>...</final>.
    Do not include any reasoning or explanation anywhere.
    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, 
    identify which answer appears most frequently and select that one.
    
    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>
    
    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

datasets_: [aime_2025, hmmt_feb_2025, gpqa]
datasets: [aime_2025]

models:

  # --- (kept) Nemotron ---
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    engine: deepspeed
    model_family: nemotron
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: []
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: ""

  # --- Qwen3 30B (A3B) ---
  - name: Qwen3-30B-A3B-Thinking-2507
    hf_repo: Qwen/Qwen3-30B-A3B-Thinking-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 4
    card: { params_B: 30.5, layers: 48, hidden_dim: 2048, heads: 32, arch: decoder-only MoE (128E, 8 active; GQA 32Q×4KV) }
    think_budgets: [32, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; thinking mode (<think>…)." # :contentReference[oaicite:2]{index=2}

  - name: Qwen3-30B-A3B-Instruct-2507
    hf_repo: Qwen/Qwen3-30B-A3B-Instruct-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 4
    card: { params_B: 30.5, layers: 48, hidden_dim: 2048, heads: 32, arch: decoder-only MoE (128E, 8 active; GQA 32Q×4KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; instruct defaults." # :contentReference[oaicite:3]{index=3}

  # --- Qwen3 4B ---
  - name: Qwen3-4B-Thinking-2507
    hf_repo: Qwen/Qwen3-4B-Thinking-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; thinking mode." # :contentReference[oaicite:4]{index=4}

  - name: Qwen3-4B-Instruct-2507
    hf_repo: Qwen/Qwen3-4B-Instruct-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [0, 256, 512, 1024, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "" # :contentReference[oaicite:5]{index=5}

  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-20b
    hf_repo: openai/gpt-oss-20b
    engine: deepspeed
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}

  # --- DeepSeek-R1 distilled to Qwen3-8B ---
  - name: DeepSeek-R1-0528-Qwen3-8B
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: deepspeed
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}

  # --- Llama 3.1 (public mirrors for config fields) ---
  - name: Llama-3.1-8B-Instruct
    hf_repo: meta-llama/Llama-3.1-8B-Instruct
    engine: deepspeed
    model_family: llama
    batch_size: 128
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Fields taken from Meta-Llama-3.1-8B mirrors (identical arch dims)." # :contentReference[oaicite:10]{index=10}

  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Llama-4-Scout-17B-16E-Instruct
    hf_repo: meta-llama/Llama-4-Scout-17B-16E-Instruct
    engine: deepspeed
    model_family: llama
    batch_size: 8
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (16 experts; vision tower present) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; Scout is 17B with 16E (official docs)." # :contentReference[oaicite:12]{index=12}

  - name: Llama-4-Maverick-17B-128E-Instruct
    hf_repo: meta-llama/Llama-4-Maverick-17B-128E-Instruct
    engine: deepspeed
    model_family: llama
    batch_size: 4
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (128 experts) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; 128E per model naming; multimodal text+vision." # :contentReference[oaicite:13]{index=13}

  # --- Gemma 3 (gated; values from official or mirrored configs) ---
  - name: gemma-3-1b-it
    hf_repo: google/gemma-3-1b-it
    engine: deepspeed
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}

  - name: gemma-3-4b-it
    hf_repo: google/gemma-3-4b-it
    engine: deepspeed
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}

  - name: gemma-3-12b-it
    hf_repo: google/gemma-3-12b-it
    engine: deepspeed
    model_family: gemma
    batch_size: 32
    card: { params_B: 12, layers: 40, hidden_dim: 3072, heads: 24, arch: gemma3_text (decoder-only) }
    think_budgets: [0, 512, 2048, 8192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:16]{index=16}

  - name: gemma-3-27b-it
    hf_repo: google/gemma-3-27b-it
    engine: deepspeed
    model_family: gemma
    batch_size: 8
    card: { params_B: 27, layers: 62, hidden_dim: 5376, heads: 32, arch: gemma3_text (decoder-only; kv_heads=16) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via MLX community mirror; official repo gated." # :contentReference[oaicite:17]{index=17}

  # --- Gemma 3N (edge, selective activation) ---
  - name: gemma-3n-E2B-it
    hf_repo: google/gemma-3n-E2B-it
    engine: deepspeed
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}

  - name: gemma-3n-E4B-it
    hf_repo: google/gemma-3n-E4B-it
    engine: deepspeed
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}

  # --- Microsoft Phi-4 family ---
  - name: phi-4
    hf_repo: microsoft/phi-4
    engine: deepspeed
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}

  - name: Phi-4-mini-reasoning
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: deepspeed
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}

  - name: Phi-4-reasoning-plus
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: deepspeed
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}

# Large models (>40B parameters) - excluded from calculations for now
_models_excluded:

  # --- Qwen3 235B (A22B) ---
  - name: Qwen3-235B-A22B-Thinking-2507
    hf_repo: Qwen/Qwen3-235B-A22B-Thinking-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Native thinking mode (<think>…); 262k context; enable vLLM: --enable-reasoning --reasoning-parser deepseek_r1." # :contentReference[oaicite:0]{index=0}

  - name: Qwen3-235B-A22B-Instruct-2507
    hf_repo: Qwen/Qwen3-235B-A22B-Instruct-2507
    engine: deepspeed
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; non-thinking (no <think>) variant." # :contentReference[oaicite:1]{index=1}

  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-120b
    hf_repo: openai/gpt-oss-120b
    engine: deepspeed
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}

  # --- GLM-4.5 Air (MoE) ---
  - name: GLM-4.5-Air
    hf_repo: zai-org/GLM-4.5-Air
    engine: deepspeed
    model_family: glm
    batch_size: 2
    card: { params_B: 106, layers: 46, hidden_dim: 4096, heads: 96, arch: decoder-only MoE (n_routed_experts=128, 8 active) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Hybrid reasoning modes; 131k context per config." # :contentReference[oaicite:9]{index=9}

  # --- Llama 3.1 (public mirrors for config fields) ---
  - name: Llama-3.1-70B-Instruct
    hf_repo: meta-llama/Llama-3.1-70B-Instruct
    engine: deepspeed
    model_family: llama
    batch_size: 2
    card: { params_B: 70, layers: 80, hidden_dim: 8192, heads: 64, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Fields from Llama-3.1-70B public config mirrors." # :contentReference[oaicite:11]{index=11}