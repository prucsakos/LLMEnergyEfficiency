# Example configuration demonstrating multiple prompt sets
# This will create separate runs for each prompt set

# Default prompts (used as fallback)
prompts:
  cot_think: |
    Solve the problem. Show your reasoning ONLY inside <scratchpad>...</scratchpad>.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <scratchpad>
  plan_think: |
    Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <plan>
  answer: |
    Using the information below, output ONLY the final answer inside <final>...</final>.
    Do not write anything outside the tags.
    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement:
  direct: |
    Answer the question. Output ONLY inside <final>...</final>.
    Do not include any reasoning or explanation anywhere.
    <question>
    {question}
    </question>
    <final>

# Multiple prompt sets to test
prompt_sets:
  - name: "robust_thorough"
    cot_think: |
      Solve this problem step by step. Think carefully and reason until you are certain of your solution.
      Show your complete reasoning process ONLY inside <scratchpad>...</scratchpad>.
      Do not write anything outside the allowed tags.
      <question>
      {question}
      </question>
      <scratchpad>
    plan_think: |
      Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
      Do not write anything outside the allowed tags.
      <question>
      {question}
      </question>
      <plan>
    answer: |
      Based on your reasoning above, provide ONLY the final answer inside <final>...</final>.
      Be precise and compact. Do not include any explanation or reasoning.
      <question>
      {question}
      </question>
      {deliberate}
      <final>
    self_eval: |
      You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
      <question>{question}</question>
      <gold>{gold}</gold>
      <candidate>{candidate}</candidate>
      Judgement:
    direct: |
      Answer the question. Output ONLY inside <final>...</final>.
      Do not include any reasoning or explanation anywhere.
      <question>
      {question}
      </question>
      <final>

  - name: "robust_analytical_pro"
    cot_think: |
      You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.
      Document your analytical process ONLY inside <scratchpad>...</scratchpad>.
      Do not write anything outside the allowed tags.
      <question>
      {question}
      </question>
      <scratchpad>
    plan_think: |
      Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
      Do not write anything outside the allowed tags.
      <question>
      {question}
      </question>
      <plan>
    answer: |
      Based on your analysis above, provide ONLY the final answer inside <final>...</final>.
      Be precise and compact. Do not include any explanation or reasoning.
      <question>
      {question}
      </question>
      {deliberate}
      <final>
    self_eval: |
      You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
      <question>{question}</question>
      <gold>{gold}</gold>
      <candidate>{candidate}</candidate>
      Judgement:
    direct: |
      Answer the question. Output ONLY inside <final>...</final>.
      Do not include any reasoning or explanation anywhere.
      <question>
      {question}
      </question>
      <final>

datasets: [mmlu]

models:
  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: Context Window 4096

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [0, 256, 2048]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: Context Window 4096
