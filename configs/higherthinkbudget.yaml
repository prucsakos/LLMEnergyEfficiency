prompts:
  cot_think: |
    Reason through this problem step by step. Continue your analysis until your reasoning converges on a clear, definitive answer. 
    State your final answer explicitly at the end.
    
    {question}
    
    Thinking: 
  answer: |
    What is the final answer? Provide only the final answer in \boxed{{}}, followed by a brief explanation only after.

    {question}

    {deliberate}

    Final answer is 
  llm_judge: |
    You are an expert evaluator. Your job is to determine if the student found the correct answer, regardless of formatting or extra text.
    
    Gold standard answer: {gold}
    Student response: {candidate}
    Question: {question}
    
    EVALUATION CRITERIA:
    - Extract the core answer from the student's response
    - Ignore formatting (bold, boxes, extra text, commentary)
    - For numbers: Same value (16 = 16, 3.14 ≈ 3.14159)  
    - For multiple choice: Same letter (A = A, regardless of explanation)
    - For expressions: Mathematically equivalent (2x = x+x)
    
    IGNORE completely:
    - Markdown formatting (**bold**, \boxed{{}}, etc.)
    - Extra explanations or commentary
    - "Wait, let me recalculate..." type additions
    - Different presentation styles
    
    Focus ONLY on: Did the student get the right mathematical answer?
    
    Response format: CORRECT or INCORRECT

    Judgement: 
  consistency_eval: |
      You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

      COUNTING RULES:
      - Count exact-string matches after trimming whitespace.
      - If there is a tie, pick the earliest occurring answer in the provided list.
      - Output the chosen answer directly without any additional formatting.

      Question:
      {question}
      
      Candidate Answers:
      {candidate_answers}

      Count the frequency of each answer and select the most popular one. Output only the chosen answer:

config_name: "smalls"
datasets_: [aime_2025, hmmt_feb_2025, gpqa]
datasets: [aime_2025]

models:
  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-20b-reason:high
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: "decoder-only moe", release_date: "2025-08-05" }
    think_budgets: [130000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation:
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs:
        reasoning_effort: "high"
      frequency_penalty: 0.005
      temperature: 1.0
      top_p: 1.0
      top_k: 0
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128."

  - name: gpt-oss-20b-reason:high
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: "decoder-only moe", release_date: "2025-08-05" }
    think_budgets: [130000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation:
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs:
        reasoning_effort: "high"
      frequency_penalty: 0.01
      temperature: 1.0
      top_p: 1.0
      top_k: 0
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128."

  - name: gpt-oss-20b-reason:high
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: "decoder-only moe", release_date: "2025-08-05" }
    think_budgets: [130000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation:
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs:
        reasoning_effort: "high"
      frequency_penalty: 0.05
      temperature: 1.0
      top_p: 1.0
      top_k: 0
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128."

  - name: gpt-oss-20b-reason:high
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: "decoder-only moe", release_date: "2025-08-05" }
    think_budgets: [130000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation:
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs:
        reasoning_effort: "high"
      frequency_penalty: 0.1
      temperature: 1.0
      top_p: 1.0
      top_k: 0
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128."


asd:
  # --- (kept) Nemotron ---
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    engine: vllm
    model_family: nemotron
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [60000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: 
      generation_mode: "chat"
      system_prompt: "/think You are an expert math tutor."
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.95
      top_k: 50
      do_sample: true
      max_new_tokens: 512
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: ""
    
  # --- DeepSeek-R1 distilled to Qwen3-8B ---
  - name: DeepSeek-R1-0528-Qwen3-8B
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [60000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: 
      generation_mode: "chat"
      system_prompt: "You are a top mathematician. Solve the problem with detailed reasoning, then give the final answer."
      chat_template_kwargs: { enable_thinking: true }
      temperature: 0.6
      top_p: 0.95
      top_k: 20
      do_sample: true
      max_new_tokens: 512
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}

  # --- Microsoft Phi-4 family ---
  - name: phi-4
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [60000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: 
      generation_mode: "chat"
      system_prompt: "Reasoning: high. You are a careful and logical assistant. Think step by step."
      chat_template_kwargs: null
      temperature: 0.8
      top_p: 0.95
      top_k: 50
      do_sample: true
      max_new_tokens: 32000
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}

  - name: Phi-4-reasoning-plus
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [60000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: 
      generation_mode: "chat"
      system_prompt: "Reasoning: high. You are a careful and logical assistant. Think step by step."
      chat_template_kwargs: null
      temperature: 0.8
      top_p: 0.95
      top_k: 50
      do_sample: true
      max_new_tokens: 32000
    reasoning: { style: single_pass, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}

# Large models (>40B parameters) - excluded from calculations for now
_models_excluded:

  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Llama-4-Scout-17B-16E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Scout-17B-16E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 8
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (16 experts; vision tower present) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 25,  # Reduced since AWQ uses less memory
     }
    generation: 
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.9
      max_new_tokens: 512
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via Unsloth config; Scout is 17B with 16E (official docs)." # :contentReference[oaicite:12]{index=12}

  - name: Llama-4-Maverick-17B-128E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Maverick-17B-128E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 4
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (128 experts) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 25,  # Reduced since AWQ uses less memory
     }
    generation: 
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.9
      max_new_tokens: 512
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via Unsloth config; 128E per model naming; multimodal text+vision." # :contentReference[oaicite:13]{index=13}

  # --- Qwen3 235B (A22B) ---
  - name: Qwen3-235B-A22B-Thinking-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq_marlin",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: 
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs: { enable_thinking: true }
      temperature: 0.6
      top_p: 0.95
      top_k: 20
      max_new_tokens: 512
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Native thinking mode (<think>…); 262k context; enable vLLM: --enable-reasoning --reasoning-parser deepseek_r1." # :contentReference[oaicite:0]{index=0}

  - name: Qwen3-235B-A22B-Instruct-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq_marlin",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: 
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs: null
      temperature: 0.7
      top_p: 0.8
      top_k: 20
      max_new_tokens: 512
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "262k context; non-thinking (no <think>) variant." # :contentReference[oaicite:1]{index=1}

  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: 
      generation_mode: "chat"
      system_prompt: "Reasoning: high"
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.95
      max_new_tokens: 20000
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}

  # --- GLM-4.5 Air (MoE) ---
  - name: GLM-4.5-Air-AWQ-4bit
    hf_repo: cpatonn/GLM-4.5-Air-AWQ-4bit
    engine: vllm
    model_family: glm
    batch_size: 2
    card: { params_B: 106, layers: 46, hidden_dim: 4096, heads: 96, arch: decoder-only MoE (n_routed_experts=128, 8 active) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: 
      generation_mode: "chat"
      system_prompt: null
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.95
      max_new_tokens: 512
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Hybrid reasoning modes; 131k context per config." # :contentReference[oaicite:9]{index=9}

  # --- Llama 3.1 (public mirrors for config fields) ---
  - name: Llama-3.1-70B-Instruct-bitsandbytes-4bit
    hf_repo: unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit
    engine: vllm
    model_family: llama
    batch_size: 2
    card: { params_B: 70, layers: 80, hidden_dim: 8192, heads: 64, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [0, 512, 2048, 8192, 12000, 16192]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 80,  # Reduced since AWQ uses less memory
       swap_space: 60,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: 
      generation_mode: "chat"
      system_prompt: "You are a brilliant mathematician who explains your work."
      chat_template_kwargs: null
      temperature: 0.6
      top_p: 0.9
      max_new_tokens: 4096
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Fields from Llama-3.1-70B public config mirrors." # :contentReference[oaicite:11]{index=11}
