# Hypothesis 1: Compute Allocation
# For fixed FLOPs, small+more reasoning can rival big+less reasoning, with different latency/memory trade-offs
# This config tests the trade-off between model size and reasoning depth

config_name: "bigones"

prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing (not even whitespace) may appear before <think> or after </think>.
    - The block MUST be non-empty, closed properly, and contain only reasoning steps.
    - Do NOT restate the question or include the final answer here.
    - Keep the reasoning concise (≤150 tokens), no filler, no meta-comments.

    <question>
    {question}
    </question>
    <think>
  plan_think: |
    Devise a brief high-level plan ONLY inside <think>...</think>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing may appear before <think> or after </think>.
    - The block MUST be non-empty and contain 1–5 short bullet points.
    - Do NOT include calculations, reasoning chains, or the final answer here.

    <question>
    {question}
    </question>
    <think>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Allowed text: <final>, </final>, Short answer string.
    - Nothing may appear before <final> or after </final>.
    - After <final>, write down the exact short answer and the closing </final> tag.

    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |-
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.

    RULES:
    - Compare trimmed strings exactly, case-sensitive.
    - If either side is empty, ambiguous, or contains multiple answers, reply NO.
    - Output MUST be non-empty: write YES or NO after "Judgement: ".

    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement: 
  direct: |
    Answer the question. Output ONLY inside <final>...</final>. Do not include any reasoning or explanation anywhere.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Nothing may appear before <final> or after </final>.
    - The block MUST be non-empty and contain only the minimal answer string.

    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

    COUNTING RULES:
    - Count exact-string matches after trimming whitespace.
    - If there is a tie, pick the earliest occurring answer in the provided list.
    - Your ENTIRE output must be EXACTLY one well-formed <chosen>...</chosen> block, containing the chosen answer copied verbatim.
    - Nothing may appear before <chosen> or after </chosen>.

    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>

    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

datasets_: [aime_2025, hmmt_feb_2025, gpqa]
datasets: [aime_2025, hmmt_feb_2025]

# Large models (>40B parameters) - excluded from calculations for now
models:

  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Llama-4-Scout-17B-16E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Scout-17B-16E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 8
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (16 experts; vision tower present) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 25,  # Reduced since AWQ uses less memory
     }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; Scout is 17B with 16E (official docs)." # :contentReference[oaicite:12]{index=12}

  - name: Llama-4-Maverick-17B-128E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Maverick-17B-128E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 4
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (128 experts) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 25,  # Reduced since AWQ uses less memory
     }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; 128E per model naming; multimodal text+vision." # :contentReference[oaicite:13]{index=13}

  - name: Qwen3-235B-A22B-Instruct-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq_marlin",  # Specify AWQ quantization
       cpu_offload_gb: 70,  # Reduced since AWQ uses less memory
       swap_space: 20,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; non-thinking (no <think>) variant." # :contentReference[oaicite:1]{index=1}

  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 70,  # Reduced since AWQ uses less memory
       swap_space: 20,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}

  # --- GLM-4.5 Air (MoE) ---
  - name: GLM-4.5-Air-AWQ-4bit
    hf_repo: cpatonn/GLM-4.5-Air-AWQ-4bit
    engine: vllm
    model_family: glm
    batch_size: 2
    card: { params_B: 106, layers: 46, hidden_dim: 4096, heads: 96, arch: decoder-only MoE (n_routed_experts=128, 8 active) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 70,  # Reduced since AWQ uses less memory
       swap_space: 20,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Hybrid reasoning modes; 131k context per config." # :contentReference[oaicite:9]{index=9}

  # --- Llama 3.1 (public mirrors for config fields) ---
  - name: Llama-3.1-70B-Instruct-bitsandbytes-4bit
    hf_repo: unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit
    engine: vllm
    model_family: llama
    batch_size: 2
    card: { params_B: 70, layers: 80, hidden_dim: 8192, heads: 64, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "bitsandbytes",  # Specify AWQ quantization
       cpu_offload_gb: 40,  # Reduced since AWQ uses less memory
       swap_space: 20,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Fields from Llama-3.1-70B public config mirrors." # :contentReference[oaicite:11]{index=11}

      # --- Qwen3 235B (A22B) ---
  - name: Qwen3-235B-A22B-Thinking-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [2048]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq_marlin",  # Specify AWQ quantization
       cpu_offload_gb: 70,  # Reduced since AWQ uses less memory
       swap_space: 20,        # CPU RAM as Virtual VRAM
       block_size: 16
     }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Native thinking mode (<think>…); 262k context; enable vLLM: --enable-reasoning --reasoning-parser deepseek_r1." # :contentReference[oaicite:0]{index=0}
