prompts:
  cot_think: |
    Think through this problem step by step. Show your reasoning, calculations, and analysis.
    Work through the problem systematically until you reach a clear, confident conclusion.
    
    {question}
    
    Thinking:
  answer: |
    Based on your reasoning, what is the final answer? Provide only the final answer in \boxed{{}}, followed by a brief explanation only after.

    {question}

    {deliberate}

    Final answer: 
  llm_judge: |
    You are an expert evaluator. Your job is to determine if the student found the correct answer, regardless of formatting or extra text.
    
    Gold standard answer: {gold}
    Student response: {candidate}
    Question: {question}
    
    EVALUATION CRITERIA:
    - Extract the core answer from the student's response
    - Ignore formatting (bold, boxes, extra text, commentary)
    - For numbers: Same value (16 = 16, 3.14 ≈ 3.14159)  
    - For multiple choice: Same letter (A = A, regardless of explanation)
    - For expressions: Mathematically equivalent (2x = x+x)
    
    IGNORE completely:
    - Markdown formatting (**bold**, \boxed{{}}, etc.)
    - Extra explanations or commentary
    - "Wait, let me recalculate..." type additions
    - Different presentation styles
    
    Focus ONLY on: Did the student get the right mathematical answer?
    
    Response format: CORRECT or INCORRECT

    Judgement: 
  consistency_eval: |
      You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

      COUNTING RULES:
      - Count exact-string matches after trimming whitespace.
      - If there is a tie, pick the earliest occurring answer in the provided list.
      - Output the chosen answer directly without any additional formatting.

      Question:
      {question}
      
      Candidate Answers:
      {candidate_answers}

      Count the frequency of each answer and select the most popular one. Output only the chosen answer:

config_name: "bigones"
datasets_: [aime_2025, hmmt_feb_2025, gpqa]
datasets: [aime_2025, hmmt_feb_2025]

# Large models (>40B parameters) - excluded from calculations for now
models:
  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Meta-Llama-3.1-70B-Instruct-bnb-4bit
    hf_repo: unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit
    engine: vllm
    model_family: llama
    batch_size: 8
    card: { params_B: 70, layers: 80, hidden_dim: 8192, heads: 64, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.9, max_model_len: 20100, enforce_eager: true,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Dims via Unsloth config; Scout is 17B with 16E (official docs)." # :contentReference[oaicite:12]{index=12}

  # --- GLM-4.5 Air (MoE) ---
  - name: GLM-4.5-Air-FP4
    hf_repo: OnFinanceAI/GLM-4.5-Air-FP4
    engine: vllm
    model_family: glm
    batch_size: 2
    card: { params_B: 106, layers: 46, hidden_dim: 4096, heads: 96, arch: decoder-only MoE (n_routed_experts=128, 8 active) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.9, enforce_eager: true, max_model_len: 20100,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Hybrid reasoning modes; 131k context per config." # :contentReference[oaicite:9]{index=9}


  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-120b-BitsAndBytes-4bit # TODO: vLLM does not support this MoE with BnB
    hf_repo: unsloth/gpt-oss-120b-unsloth-bnb-4bit
    engine: vllm
    model_family: gpt
    batch_size: 1
    card: { params_B: 117, layers: 36, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (128E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.9, enforce_eager: true, max_model_len: 20100,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Harmony chat template required; 131k max_position; MoE MXFP4 weights." # :contentReference[oaicite:7]{index=7}

  - name: Llama-4-Maverick-17B-128E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Maverick-17B-128E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 4
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (128 experts) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.8, enforce_eager: true, max_model_len: 20100,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "17B param model with 128 experts" # :contentReference[oaicite:13]{index=13}

  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Llama-4-Scout-17B-16E-Instruct-AWQ
    hf_repo: kishizaki-sci/Llama-4-Scout-17B-16E-Instruct-AWQ
    engine: vllm
    model_family: llama
    batch_size: 8
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (16 experts; vision tower present) }
    think_budgets: [20000]
    backend: { 
       dtype: "auto",  # Let vLLM auto-detect for quantized models
       gpu_memory_utilization: 0.95, 
       enforce_eager: true,
       quantization: "awq",  # Specify AWQ quantization
       cpu_offload_gb: 25,  # Reduced since AWQ uses less memory
     }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "17B param model with 16 experts" # :contentReference[oaicite:12]{index=12}


  - name: Qwen3-235B-A22B-Instruct-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.9, enforce_eager: true, max_model_len: 20100,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "262k context; non-thinking (no <think>) variant." # :contentReference[oaicite:1]{index=1}

      # --- Qwen3 235B (A22B) ---
  - name: Qwen3-235B-A22B-Thinking-2507-AWQ
    hf_repo: QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ
    engine: vllm
    model_family: qwen
    batch_size: 1
    card: { params_B: 235, layers: 94, hidden_dim: 4096, heads: 64, arch: decoder-only MoE (128E, 8 active; GQA 64Q×4KV) }
    think_budgets: [20000]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.9, enforce_eager: true, max_model_len: 20100,
       #quantization: "bitsandbytes", 
       #cpu_offload_gb: 40, swap_space: 20, block_size: 16
        }
    generation: { temperature: 0.8, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true, openai_eval: true }
    notes: "Native thinking mode (<think>…); 262k context; enable vLLM: --enable-reasoning --reasoning-parser deepseek_r1." # :contentReference[oaicite:0]{index=0}
