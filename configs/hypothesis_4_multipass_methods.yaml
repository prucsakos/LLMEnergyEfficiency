# Hypothesis 4: Multi-pass Methods (deliberate/SC)
# Nearly linear compute increase; benefits are task-dependent (e.g., strong on math word problems)
# This config tests the linear scaling of compute with multi-pass methods and their task-specific benefits

config_name: "hypothesis_4_multipass_methods"

prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.
    Document your analytical process ONLY inside <scratchpad>...</scratchpad>.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <scratchpad>
  plan_think: |
    Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <plan>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.
    Be precise and compact. Do not include any explanation or reasoning.
    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement:
  direct: |
    Answer the question. Output ONLY inside <final>...</final>.
    Do not include any reasoning or explanation anywhere.
    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, 
    identify which answer appears most frequently and select that one.
    
    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>
    
    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

datasets: [gsm8k]

models:
  # Test linear scaling of self-consistency with different K values
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 1, self_eval: true }  # K=1 (baseline)
    notes: Self-consistency K=1 (baseline)

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 3, self_eval: true }  # K=3
    notes: Self-consistency K=3 (3x compute)

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 5, self_eval: true }  # K=5
    notes: Self-consistency K=5 (5x compute)

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 7, self_eval: true }  # K=7
    notes: Self-consistency K=7 (7x compute)

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 10, self_eval: true }  # K=10
    notes: Self-consistency K=10 (10x compute)

  # Test with different model to see if linear scaling holds
  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 1, self_eval: true }  # K=1 (baseline)
    notes: Self-consistency K=1 with Llama2 (baseline)

  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 5, self_eval: true }  # K=5
    notes: Self-consistency K=5 with Llama2 (5x compute)

  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 10, self_eval: true }  # K=10
    notes: Self-consistency K=10 with Llama2 (10x compute)

  # Test plan style vs cot style to see task-dependent benefits
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [512]  # Fixed reasoning budget
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: plan, self_consistency_k: 5, self_eval: true }  # Plan style
    notes: Self-consistency K=5 with plan style (task-dependent benefits)
