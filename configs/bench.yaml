prompts:
  cot_think: |
    Solve the problem. Show your reasoning ONLY inside <scratchpad>...</scratchpad>.
    <question>
    {question}
    </question>
    <scratchpad>
  plan_think: |
    Devise a short plan ONLY inside <plan>...</plan>.
    <question>
    {question}
    </question>
    <plan>
  answer: |
    Using the information below, output ONLY the final answer inside <final>...</final>.
    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement:
  direct: |
    Answer the question. Output ONLY inside <final>...</final>.
    <question>{question}</question>
    <final>

models:
  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: Llama2-13b-chat
    hf_repo: meta-llama/Llama-2-13b-chat-hf
    batch_size: 64
    card: { params_B: 13, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.55, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: Llama3-8b-instruct
    hf_repo: meta-llama/Meta-Llama-3-8B-Instruct
    batch_size: 96
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.5, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: mistral-7b
    hf_repo: mistralai/Mistral-7B-v0.1
    batch_size: 128
    card: { params_B: 7.3, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: mistral-7b-openorca
    hf_repo: Open-Orca/Mistral-7B-OpenOrca
    batch_size: 128
    card: { params_B: 7.3, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: wizardmath-7b-v1.1
    hf_repo: WizardLMTeam/WizardMath-7B-V1.1
    batch_size: 96
    card: { params_B: 7.3, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.5, enforce_eager: true }
    generation: { temperature: 0.0, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: falcon-7b-instruct
    hf_repo: tiiuae/falcon-7b-instruct
    batch_size: 96
    card: { params_B: 7, layers: 32, hidden_dim: 4544, heads: 71, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: mpt-7b-instruct
    hf_repo: mosaicml/mpt-7b-instruct
    batch_size: 128
    card: { params_B: 6.7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: xgen-7b-8k-inst
    hf_repo: Salesforce/xgen-7b-8k-inst
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: qwen-7b-chat
    hf_repo: Qwen/Qwen-7B-Chat
    batch_size: 96
    card: { params_B: 7.7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.2, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }

  - name: openorca-platypus2-13b
    hf_repo: Open-Orca/OpenOrca-Platypus2-13B
    batch_size: 64
    card: { params_B: 13, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only }
    think_budgets: [0, 256, 2048]
    datasets: [mmlu, gsm8k, csqa]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.65, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }