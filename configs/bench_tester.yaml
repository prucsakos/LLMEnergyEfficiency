config_name: "prompt_checking_4"

prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing (not even whitespace) may appear before <think> or after </think>.
    - The block MUST be non-empty, closed properly, and contain only reasoning steps.
    - Do NOT restate the question or include the final answer here.
    - Keep the reasoning concise (≤150 tokens), no filler, no meta-comments.

    <question>
    {question}
    </question>
    <think>
  plan_think: |
    Devise a brief high-level plan ONLY inside <think>...</think>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <think>...</think> block.
    - Nothing may appear before <think> or after </think>.
    - The block MUST be non-empty and contain 1–5 short bullet points.
    - Do NOT include calculations, reasoning chains, or the final answer here.

    <question>
    {question}
    </question>
    <think>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Allowed text: <final>, </final>, Short answer string.
    - Nothing may appear before <final> or after </final>.
    - After <final>, write down the exact short answer and the closing </final> tag.

    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |-
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.

    RULES:
    - Compare trimmed strings exactly, case-sensitive.
    - If either side is empty, ambiguous, or contains multiple answers, reply NO.
    - Output MUST be non-empty: write YES or NO after "Judgement: ".

    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement: 
  direct: |
    Answer the question. Output ONLY inside <final>...</final>. Do not include any reasoning or explanation anywhere.

    HARD FORMAT RULES:
    - Your ENTIRE output must be EXACTLY one well-formed <final>...</final> block.
    - Nothing may appear before <final> or after </final>.
    - The block MUST be non-empty and contain only the minimal answer string.

    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, identify which answer appears most frequently and select that one.

    COUNTING RULES:
    - Count exact-string matches after trimming whitespace.
    - If there is a tie, pick the earliest occurring answer in the provided list.
    - Your ENTIRE output must be EXACTLY one well-formed <chosen>...</chosen> block, containing the chosen answer copied verbatim.
    - Nothing may appear before <chosen> or after </chosen>.

    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>

    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

datasets: [aime_2025]

models:
  # --- (kept) Nemotron ---
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    engine: vllm
    model_family: nemotron
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: [256]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: ""

models_:

  # --- (kept) Nemotron ---
  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    engine: vllm
    model_family: nemotron
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: []
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: ""

  # --- Qwen3 30B (A3B) ---
  - name: Qwen3-30B-A3B-Thinking-2507
    hf_repo: Qwen/Qwen3-30B-A3B-Thinking-2507
    engine: vllm
    model_family: qwen
    batch_size: 4
    card: { params_B: 30.5, layers: 48, hidden_dim: 2048, heads: 32, arch: decoder-only MoE (128E, 8 active; GQA 32Q×4KV) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; thinking mode (<think>…)." # :contentReference[oaicite:2]{index=2}

  - name: Qwen3-30B-A3B-Instruct-2507
    hf_repo: Qwen/Qwen3-30B-A3B-Instruct-2507
    engine: vllm
    model_family: qwen
    batch_size: 4
    card: { params_B: 30.5, layers: 48, hidden_dim: 2048, heads: 32, arch: decoder-only MoE (128E, 8 active; GQA 32Q×4KV) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; instruct defaults." # :contentReference[oaicite:3]{index=3}

  # --- Qwen3 4B ---
  - name: Qwen3-4B-Thinking-2507
    hf_repo: Qwen/Qwen3-4B-Thinking-2507
    engine: vllm
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "262k context; thinking mode." # :contentReference[oaicite:4]{index=4}

  - name: Qwen3-4B-Instruct-2507
    hf_repo: Qwen/Qwen3-4B-Instruct-2507
    engine: vllm
    model_family: qwen
    batch_size: 128
    card: { params_B: 4.0, layers: 36, hidden_dim: 2560, heads: 32, arch: decoder-only (GQA 32Q×8KV) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.7, top_p: 0.8, top_k: 20, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "" # :contentReference[oaicite:5]{index=5}

  # --- OpenAI GPT-OSS (Harmony format) ---
  - name: gpt-oss-20b
    hf_repo: openai/gpt-oss-20b
    engine: vllm
    model_family: gpt
    batch_size: 8
    card: { params_B: 21.5, layers: 24, hidden_dim: 2880, heads: 64, arch: decoder-only MoE (32E, 4 active; sliding+full attention; YARN RoPE; KV heads=8) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Use tokenizer.apply_chat_template (Harmony) or openai-harmony; 131k max_position; sliding_window=128." # :contentReference[oaicite:6]{index=6}

  # --- DeepSeek-R1 distilled to Qwen3-8B ---
  - name: DeepSeek-R1-0528-Qwen3-8B
    hf_repo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
    engine: vllm
    model_family: deepseek
    batch_size: 64
    card: { params_B: 8.2, layers: 36, hidden_dim: 4096, heads: 32, arch: decoder-only (Qwen3; GQA 32Q×8KV) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Same arch as Qwen3-8B; recommended T=0.6, top_p=0.95; tokenizer from DeepSeek repo." # :contentReference[oaicite:8]{index=8}

  # --- Llama 3.1 (public mirrors for config fields) ---
  - name: Llama-3.1-8B-Instruct
    hf_repo: meta-llama/Llama-3.1-8B-Instruct
    engine: vllm
    model_family: llama
    batch_size: 128
    card: { params_B: 8, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only (GQA kv_heads=8) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Fields taken from Meta-Llama-3.1-8B mirrors (identical arch dims)." # :contentReference[oaicite:10]{index=10}

  # --- Llama 4 (public config mirrors; multimodal MoE) ---
  - name: Llama-4-Scout-17B-16E-Instruct
    hf_repo: meta-llama/Llama-4-Scout-17B-16E-Instruct
    engine: vllm
    model_family: llama
    batch_size: 8
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (16 experts; vision tower present) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; Scout is 17B with 16E (official docs)." # :contentReference[oaicite:12]{index=12}

  - name: Llama-4-Maverick-17B-128E-Instruct
    hf_repo: meta-llama/Llama-4-Maverick-17B-128E-Instruct
    engine: vllm
    model_family: llama
    batch_size: 4
    card: { params_B: 17, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only multimodal MoE (128 experts) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via Unsloth config; 128E per model naming; multimodal text+vision." # :contentReference[oaicite:13]{index=13}

  # --- Gemma 3 (gated; values from official or mirrored configs) ---
  - name: gemma-3-1b-it
    hf_repo: google/gemma-3-1b-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 1, layers: 26, hidden_dim: 1152, heads: 4, arch: gemma3_text (decoder-only) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; 32k input context per card; let loader resolve dims." # :contentReference[oaicite:14]{index=14}

  - name: gemma-3-4b-it
    hf_repo: google/gemma-3-4b-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3_text (decoder-only) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:15]{index=15}

  - name: gemma-3-12b-it
    hf_repo: google/gemma-3-12b-it
    engine: vllm
    model_family: gemma
    batch_size: 32
    card: { params_B: 12, layers: 40, hidden_dim: 3072, heads: 24, arch: gemma3_text (decoder-only) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "128k input context (Gemma 3 family)." # :contentReference[oaicite:16]{index=16}

  - name: gemma-3-27b-it
    hf_repo: google/gemma-3-27b-it
    engine: vllm
    model_family: gemma
    batch_size: 8
    card: { params_B: 27, layers: 62, hidden_dim: 5376, heads: 32, arch: gemma3_text (decoder-only; kv_heads=16) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Dims via MLX community mirror; official repo gated." # :contentReference[oaicite:17]{index=17}

  # --- Gemma 3N (edge, selective activation) ---
  - name: gemma-3n-E2B-it
    hf_repo: google/gemma-3n-E2B-it
    engine: vllm
    model_family: gemma
    batch_size: 256
    card: { params_B: 2, layers: 28, hidden_dim: 1536, heads: 12, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; Together AI lists E-series selective activation for edge." # :contentReference[oaicite:18]{index=18}

  - name: gemma-3n-E4B-it
    hf_repo: google/gemma-3n-E4B-it
    engine: vllm
    model_family: gemma
    batch_size: 128
    card: { params_B: 4, layers: 32, hidden_dim: 2048, heads: 16, arch: gemma3n (multimodal; selective activation) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.95, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "Gated repo; Together AI model page for E4B." # :contentReference[oaicite:19]{index=19}

  # --- Microsoft Phi-4 family ---
  - name: phi-4
    hf_repo: microsoft/phi-4
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 14.7, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "14B dense transformer; 16k context; defaults per model card." # :contentReference[oaicite:20]{index=20}

  - name: Phi-4-mini-reasoning
    hf_repo: microsoft/Phi-4-mini-reasoning
    engine: vllm
    model_family: phi
    batch_size: 128
    card: { params_B: 4.9, layers: 32, hidden_dim: 3072, heads: 24, arch: decoder-only (dense) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "4.9B parameters; 128k context; reasoning-optimized variant." # :contentReference[oaicite:21]{index=21}

  - name: Phi-4-reasoning-plus
    hf_repo: microsoft/Phi-4-reasoning-plus
    engine: vllm
    model_family: phi
    batch_size: 32
    card: { params_B: 13.6, layers: 40, hidden_dim: 5120, heads: 40, arch: decoder-only (dense) }
    think_budgets: [128]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
    generation: { temperature: 0.6, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
    notes: "13.6B parameters; reasoning-optimized variant with enhanced capabilities." # :contentReference[oaicite:22]{index=22}
