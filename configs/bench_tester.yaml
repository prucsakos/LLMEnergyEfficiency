config_name: "bench_tester_datasets"

prompts:
  cot_think: |
    You are a worldwide professional problem solver with no mistakes. Analyze this problem systematically. Work through each step methodically until you reach a confident solution.
    Document your analytical process ONLY inside <scratchpad>...</scratchpad>.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <scratchpad>
  plan_think: |
    Devise a brief high-level plan ONLY inside <plan>...</plan>, then solve with a scratchpad.
    Do not write anything outside the allowed tags.
    <question>
    {question}
    </question>
    <plan>
  answer: |
    Based on your analysis above, provide ONLY the final answer inside <final>...</final>.
    Be precise and compact. Do not include any explanation or reasoning.
    <question>
    {question}
    </question>
    {deliberate}
    <final>
  self_eval: |
    You are a strict judge. Reply ONLY YES if the candidate answer equals the gold answer, else NO.
    <question>{question}</question>
    <gold>{gold}</gold>
    <candidate>{candidate}</candidate>
    Judgement:
  direct: |
    Answer the question. Output ONLY inside <final>...</final>.
    Do not include any reasoning or explanation anywhere.
    <question>
    {question}
    </question>
    <final>
  consistency_eval: |
    You are a majority vote counter. Given a question and multiple candidate answers, 
    identify which answer appears most frequently and select that one.
    
    <question>
    {question}
    </question>
    <candidate_answers>
    {candidate_answers}
    </candidate_answers>
    
    Count the frequency of each answer and select the most popular one by outputting ONLY the chosen answer inside <chosen>...</chosen>.
    <chosen>

datasets: [aime_2025, hmmt_feb_2025, gpqa]

models_:
- name: Llama 2 70B
  engine: vllm
  hf_repo: meta-llama/Llama-2-70b-hf
  batch_size: 128
  card: { params_B: 70, layers: 80, hidden_dim: 8192, heads: 64, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window ~4,096 tokens.

- name: Gemma 7B
  engine: vllm
  hf_repo: google/gemma-7b
  batch_size: 128
  card: { params_B: 7, layers: 28, hidden_dim: 3072, heads: 16, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window ~8,192 tokens.

- name: Nemotron‑H 47B (Reasoning‑128K)
  engine: vllm
  hf_repo: nvidia/Nemotron-H-47B-Reasoning-128K
  batch_size: 128
  card: { params_B: 46.8, layers: 98, hidden_dim: 8192, heads: 64, arch: hybrid (Mamba2+Transformer) }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window up to 128,000 tokens.

- name: AI21 Jamba 52B (base)
  engine: vllm
  hf_repo: ai21labs/Jamba-v0.1
  batch_size: 128
  card: { params_B: 52, layers: 32, hidden_dim: 4096, heads: 32, arch: hybrid (Transformer–Mamba MoE) }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window up to 256,000 tokens. (~12B active params at inference; total capacity ~52B.)

- name: Mistral 7B
  engine: vllm
  hf_repo: mistralai/Mistral-7B-v0.1
  batch_size: 128
  card: { params_B: 7.3, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Trained for 8,192‑token context (SWA with ~4k sliding window).

- name: Qwen2.5 14B
  engine: vllm
  hf_repo: Qwen/Qwen2.5-14B
  batch_size: 128
  card: { params_B: 14.7, layers: 48, hidden_dim: 5120, heads: 40, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window up to 131,072 tokens (1M‑token long‑context variants also exist).

- name: Falcon 40B
  engine: vllm
  hf_repo: tiiuae/falcon-40b
  batch_size: 128
  card: { params_B: 40, layers: 60, hidden_dim: 8192, heads: 128, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.95, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window ~2,048 tokens.

- name: GPT‑NeoX 20B
  engine: vllm
  hf_repo: EleutherAI/gpt-neox-20b
  batch_size: 128
  card: { params_B: 20, layers: 44, hidden_dim: 6144, heads: 64, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window ~2,048 tokens.

- name: StarCoder2 15B
  engine: vllm
  hf_repo: bigcode/starcoder2-15b
  batch_size: 128
  card: { params_B: 15, layers: 40, hidden_dim: 6144, heads: 48, arch: decoder-only }
  think_budgets: [512]
  backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
  generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 256 }
  reasoning: { style: cot, self_consistency_k: 0, self_eval: true }
  notes: Context window 16,384 tokens (4,096 sliding window attention).

models:
  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [256]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, top_k: 5, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 1, self_eval: true }
    notes: Context Window 4096 - strict

  - name: Llama2-7b-chat
    hf_repo: meta-llama/Llama-2-7b-chat-hf
    batch_size: 128
    card: { params_B: 7, layers: 32, hidden_dim: 4096, heads: 32, arch: decoder-only }
    think_budgets: [256]
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 2, top_p: 0.9, top_k: 30, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 1, self_eval: true }
    notes: Context Window 4096 - creative

  - name: Nemotron-Nano-9B-v2
    hf_repo: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    batch_size: 128
    card: { params_B: 8.89, layers: 56, hidden_dim: 4480, heads: 40, arch: hybrid-Mamba-Transformer }
    think_budgets: []
    backend: { dtype: bfloat16, gpu_memory_utilization: 0.45, enforce_eager: true }
    generation: { temperature: 0.1, top_p: 0.9, max_new_tokens: 512 }
    reasoning: { style: cot, self_consistency_k: 0, self_eval: true }