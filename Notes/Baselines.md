| Model                                             | Reasoning mode (if applicable) | AIME 2025 (accuracy/pass@1)        | HMMT Feb 2025                                                 | MATH‑500 (reference)         | Sources                                                                                                                                                                                                          |
| ------------------------------------------------- | ------------------------------ | ---------------------------------- | ------------------------------------------------------------- | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **GPT‑OSS‑20B (no tools)**                        | Low reasoning                  | 37.1 % (“low” reasoning)           | NaN                                                           | NaN                          | GPT‑OSS model card shows that the low‑reasoning mode solved 11/30 AIME 2025 problems (37.1 %).                                                                                                                   |
|                                                   | Medium reasoning               | 72.1 %                             | NaN                                                           | NaN                          | Same card reports 21/30 correct at medium reasoning.                                                                                                                                                             |
|                                                   | High reasoning                 | 91.7 %                             | **75.0 %** average accuracy on HMMT Feb 2025 (high reasoning) | NaN                          | High‑reasoning mode achieved 28/30 AIME 2025 problems and 75 % accuracy on the HMMT Feb 2025 math contest.                                                                                                       |
| **GPT‑OSS‑120B (bits‑and‑bytes 4‑bit)**           | High reasoning                 | **92.5 %** AIME25 (high reasoning) | NaN                                                           | **94.0 %** MATH‑500          | The GPT‑OSS model card shows the 120‑B variant achieving 92.5 % on AIME 2025 at high reasoning; Kaggle’s MATH‑500 leaderboard lists GPT‑OSS‑120B with ~94 % accuracy.  No HMMT 25 result was found.              |
| **Qwen3‑235B‑A22B‑Thinking‑2507‑AWQ**             | thinking                       | **92.3 %** AIME 25                 | **83.9 %**                                                    | NaN                          | Qwen3‑30B‑A3B/4B/235B model card lists a reasoning table; the Qwen3‑235B‑A22B‑Thinking‑2507 model gets 92.3 % on AIME 25 and 83.9 % on HMMT 25.                                                                  |
| **Qwen3‑30B‑A3B‑Thinking‑2507**                   | thinking                       | **85.0 %** AIME25                  | **71.4 %**                                                    | **93.9 %** (MATH‑500)        | Qwen3‑30B‑A3B‑Thinking‑2507’s performance table lists 85 % AIME25 and 71.4 % HMMT Feb 25.  SiliconFlow notes that the same 30B model achieves ~93.9 % on MATH‑500.                                               |
| **Qwen3‑30B‑A3B‑Instruct‑2507**                   | non‑thinking                   | **≈70.9 %** AIME25                 | **49.8 %**                                                    | **93.9 %**                   | The same table gives AIME25 ≈70.9 % and HMMT ≈49.8 % for the non‑thinking A3B model.  The SiliconFlow page reports 93.9 % on MATH‑500.                                                                           |
| **Qwen3‑4B‑Thinking‑2507**                        | thinking                       | **81.3 %** AIME25                  | **55.5 %**                                                    | NaN                          | Qwen3‑4B‑Thinking‑2507’s performance table shows 81.3 % AIME25 and 55.5 % HMMT 25.                                                                                                                               |
| **Qwen3‑4B‑Instruct‑2507**                        | non‑thinking                   | **47.4 %** AIME25                  | **31.0 %**                                                    | NaN                          | The non‑thinking Qwen3‑4B‑Instruct‑2507 card reports 47.4 % AIME25 and 31 % HMMT 25.                                                                                                                             |
| **Qwen3‑235B‑A22B**                               | non‑thinking                   | **81.5 %** AIME25                  | **62.5 %**                                                    | NaN                          | In the DeepSeek‑R1‑0528 report, the comparison table lists the base Qwen3‑235B‑A22B with 81.5 % AIME25 and 62.5 % HMMT 25.                                                                                       |
| **Qwen3‑235B‑A22B‑Instruct‑2507‑AWQ**             | instruct                       | **≈70.3 %** AIME25                 | NaN                                                           | NaN                          | LLM‑Stats’ AIME 2025 leaderboard shows the Qwen3‑235B‑A22B‑Instruct‑2507 model scoring 0.703 (≈70.3 %) pass@1; no HMMT or MATH‑500 results were found.                                                           |
| **DeepSeek‑R1‑0528‑Qwen3‑8B**                     | reasoning (distilled)          | **76.3 %** AIME25                  | **61.5 %**                                                    | NaN                          | The DeepSeek‑R1‑0528 report lists that the distilled Qwen3‑8B model achieves 76.3 % on AIME 25 and 61.5 % on HMMT Feb 25.                                                                                        |
| **GLM‑4.5‑Air‑AWQ 4‑bit**                         | thinking (hybrid)              | NaN                                | **69.17 %**                                                   | NaN                          | MathArena’s HMMT table reports “GLM 4.5 AIR” with ~69.17 % average accuracy; no AIME 25 or MATH‑500 values were found.                                                                                           |
| **Nemotron‑Nano‑9B‑v2**                           | –                              | **≈71.1 %** AIME25                 | NaN                                                           | NaN                          | The Nemotron‑Nano‑9B‑v2 technical report compares Nemotron‑Nano‑9B‑v2 to Qwen3‑8B; the figure shows that the model attains ~71.1 % on AIME 25.                                                                   |
| **Phi‑4**                                         | –                              | NaN                                | NaN                                                           | NaN                          | The Phi‑4 model card does not report AIME 2025, HMMT or MATH‑500 results.                                                                                                                                        |
| **Phi‑4‑mini‑reasoning (3.8B)**                   | reasoning                      | **31.77 %** AIME25                 | NaN                                                           | **91.20 %**                  | The mini reasoning card’s table lists pass@1 results showing 31.77 % AIME25 and 91.20 % MATH‑500 accuracy.                                                                                                       |
| **Phi‑4‑reasoning‑plus**                          | reasoning‑plus                 | **78.0 %** AIME25                  | **46.67 %**                                                   | NaN                          | The Phi‑4‑Reasoning model card reports that the “Reasoning Plus” variant achieves 78 % on AIME 25.  MathArena’s HMMT table lists Phi‑4‑Reasoning‑Plus with 46.67 % accuracy.                                     |
| **Gemma‑3n‑E2B‑IT**                               | –                              | **6.7 %** AIME25                   | NaN                                                           | NaN                          | The Gemma‑3n‑E2B‑IT model card lists a “AIME 2025 accuracy (0‑shot)” of 6.7 %.                                                                                                                                   |
| **Gemma‑3n‑E4B‑IT**                               | –                              | **11.6 %** AIME25                  | NaN                                                           | NaN                          | Gemma‑3n‑E4B‑IT card shows a 0‑shot AIME 2025 accuracy of 11.6 %.                                                                                                                                                |
| **Gemma‑3‑1B‑IT**                                 | –                              | NaN                                | NaN                                                           | NaN                          | No AIME25/HMMT/MATH‑500 results found in the model cards or technical report.                                                                                                                                    |
| **Gemma‑3‑4B‑IT**                                 | –                              | NaN                                | NaN                                                           | NaN                          | No relevant benchmark values available.                                                                                                                                                                          |
| **Gemma‑3‑12B‑IT**                                | –                              | NaN                                | NaN                                                           | NaN                          | No relevant benchmark values available.                                                                                                                                                                          |
| **Gemma‑3‑27B‑IT**                                | –                              | NaN                                | NaN                                                           | NaN                          | No relevant benchmark values available for the base model (a fine‑tuned LogicFlow variant reports 13.3 % AIME25 and 76.8 % MATH, but those figures are for a different LoRA fine‑tuning and not the base model). |
| **Llama‑4‑Scout‑17B‑16E‑Instruct‑AWQ**            | –                              | NaN                                | NaN                                                           | NaN                          | No published AIME 25, HMMT or MATH‑500 results for the Scout model.                                                                                                                                              |
| **Llama‑4‑Maverick‑17B‑128E‑Instruct‑AWQ**        | –                              | NaN                                | **8.33 %** HMMT 25                                            | NaN                          | The MathArena table lists “LLAMA‑4‑MAVERICK” with an average accuracy of 8.33 % on HMMT Feb 25.                                                                                                                  |
| **Llama‑3.1‑8B‑Instruct**                         | non‑thinking                   | NaN                                | NaN                                                           | **≈51.9 %** (MATH benchmark) | The OpenMath2 model card shows that Llama‑3.1‑8B‑Instruct obtains ≈51.9 % accuracy on the MATH benchmark (used as a proxy for MATH‑500).                                                                         |
| **Llama‑3.1‑70B‑Instruct (bits‑and‑bytes 4‑bit)** | –                              | NaN                                | NaN                                                           | **≈67.9 %**                  | The OpenMath2 card lists Llama‑3.1‑70B‑Instruct with ≈67.9 % accuracy on the MATH benchmark; no AIME25 or HMMT results were located.                                                                             |
